---
title: '基础学习记录'
date: 2023-06-17
permalink: /posts/2023/06/blog-post-self-study/
tags:
  - 科研
  - 自学
---

### ABINET + BERT + DAN + Transformer

## 6.17 学习笔记
1. 输入、输出、中间层是如何建模的？请从抽象数据类型+实现阐述建模。
2. 论文作者声明的动机是什么样子的？
3. 你抽象的建模是怎么支撑作者声明的动机的？

  将语言知识应用于场景文字识别Scene Text Recognition，提出自主、双向和迭代的语言建模方法，可以更好地利用上下文信息来提高场景文字识别的准确率。
  传统上，通常采用卷积神经网络（CNN）或递归神经网络（RNN）等深度学习模型进行特征提取和序列建模。

  现有技术面临的挑战，遮挡、模糊、噪声，视觉辨别能力不足，影响准确性。（->自主）
  传统的单向建模方式可能会丢失一些重要的上下文信息，从而导致文字识别错误。（->双向）
  vm存在噪声输入，误差影响lm精度（->迭代）

  自主/自治：视觉模型VM和语言模型LM，通过阻断视觉模型和语言模型之间的梯度流动 //注意力 基于RNNs或transformer的lm
  双向：从左到右（L2R）和从右到左（R2L）。通过反馈机制交互，使用注意力机制捕获不同层次的语义信息
  迭代：迭代训练优化模型提高精度，提出半监督学习的集成自训练方法

  VM输入图像，提取特征，输出结果到LM，融合预测结果
  堆保存已处理的字符的隐藏状态向量，队列保存下一个要处理的字符的隐藏状态向量，列表保存已生成的识别结果

## 6.21学习笔记
本周进度：
	①基础知识学习（机器学习、OCR技术概述、RCNN）
	②ABINET论文阅读（重点思考论文图片）
疑问：
	①mask为何要屏蔽自己？（类似完形填空，不能用自己推自己）
	②为什么融合用add？（为了对齐视觉特征和语言特征，相加后矩阵维度不变）
	③迭代最开始只依赖于VM输出，会不会导致最终结果误差很大？（这里我第一次看的时候理解错了，后来发现只有第一次迭代输入是VM的输出，之后迭代的输入都是上一轮融合模型的输出）

  输入 表示场景文本图像的二维矩阵
  中间层 堆或队列（处理矩阵，文本分割识别，提取字符特征，上下文信息修正）
  输出 多个连续字符组成的序列/字符串/线性表

  动机：1科学问题（聚焦的研究对象）2针对此问题的建模，挖掘/利用了什么关系（main idea）
  现象/情景：单字符 不基于上下文识别 部分存在问题
  提高场景文本识别的准确性和鲁棒性。
  现有方法常常需要手动调整网络结构或优化参数才能达到较好的识别效果（鲁棒），且没有充分利用上下文和语言规则等信息（准确），因此提出自主、双向和迭代的语言建模方法，并引入注意力机制等技术。

  描述输入输出函数：
  输入 一幅场景文本图像
  处理 根据上下文信息处理不同方向和位置上的文本；根据语言信息多次迭代，提高识别准确性。
  输出 对该图像中的文本进行识别并返回文本字符串

## 6.28学习笔记
BERT（Bidirectional Encoder Representations from Transformers）是一种基于Transformer模型的预训练语言模型
下游任务是指使用预训练好的BERT模型作为基础，在特定的任务上进行微调。
预训练：在大规模文本语料上进行无监督的预训练，从而学习通用的语义表示。
微调：在特定任务上进行有监督微调，以适应不同的下游任务。
双向上下文建模：通过使用Transformer的多层双向编码器来建模文本序列中的上下文关系。相比传统的单向模型，可以同时利用前后文信息，更全面地理解每个词的语境。
Masked Language Model（MLM）：预训练过程包括对输入文本中的某些词进行掩码，并要求模型预测这些被掩码的词。通过这种方式，学习丰富的上下文依赖关系，使得模型能够进行词语推理和填充缺失的单词。
Next Sentence Prediction（NSP）：通过预测输入文本中两个句子是否是连续的来进行训练。有助于模型理解句子之间的逻辑关系和语义关联。
多层隐藏表示：模型包含多个隐藏层，每个隐藏层都输出一组编码表示。这些多层的隐藏表示捕获了不同层次的语义信息，可以用于各种分析和可视化任务。最后一层隐藏层的输出可以用于不同任务的后续处理或特征提取。

本周进度：
  ①	基础知识学习（Transformer、NLP）
  ②	BERT论文阅读
疑问：
  ①	为什么要随机mask？（使模型学习到上下文相关的表示，同时也能避免过拟合）
  ②	隐藏向量？（encoder self-attention decoder ）
 
BERT训练中的下游任务有哪些？
  //输入输出中间层的数据结构
  //每个下游任务挖掘的什么样的关系 //得到什么样的表征 //计算式体现了输入输出之间的什么关系

BERT训练描述：
  输入：多个连续字符组成的文本序列/字符串/线性表 //高级语义 
    每一个字符/token是一个向量，具有多个特征维度（实数） 
    矩阵形式存储
  中间层：处理为一维向量（如分词、添加特殊标记、提取语义信息）
    集合形式
  输出：文本序列语义的向量/矩阵
	

常见的下游任务
1.句子对分类任务：
  输入：两个句子/两个文本序列
  输出：判断二者关系的向量/类别标签
  判断句子对之间的关系
2.单句分类任务：（类中心判断）
  输入：文本序列/字符串列表
  输出：向量/类别标签
  分析文本与所属类别之间的关系
3.问答任务：
  输入：问题和段落文本
  输出：语义向量
  判断问题和段落文本语义的关系，通过语义向量定位答案
4.单句标注任务：
  输入：文本序列/字符串列表
  输出：对应的预测标签序列
  对每个单元进行分类或标注
5.Next Sentence Prediction（NSP）任务：
  输入：两个文本序列
  输出：true或false
  判断两个句子是否连续

## 7.19学习记录
Decoupled Attention Network(DAN)解耦注意力网络，该网络将注意力的对齐阶段从解码器中解耦出来，即进行对齐时不再依赖于上一步的解码信息。
然而，当前注意力机制的对齐操作依赖于上一步的解码信息，一旦上一步解码出错或具有迷惑性，将产生错误，且会累积传播。（注意力参数依靠序列产生）这一问题在较长的手写文本上体现得较为明显。
模块组成：特征提取器（FE）、卷积对齐模块（CAM）、去耦解码器（DTD）。FE对输入图片提取多个尺度的特征图；CAM接收特征提取器中的多尺度特征，采用全卷积结构，输出与特征图等尺寸的attention map；最后DTD解码出识别结果。
通过卷积对齐模块（CAM），成功学习到字符图像不同字符的注意力权重，并先于循环神经网络加入注意力机制，成功解决了经典注意力机制因序列重复或相似带来的对齐问题。CAM的输入是来自特征编码器的每个尺度的视觉特征。
但也存在一定的问题：仅仅根据视觉信息判断注意力，这会在噪声较大的图像很难取得好的识别率，因为没有前后文信息作为参考，使得错检。
1D(规则的手写文本)和2D(不规则场景文本)

本周进度：
  1.BERT和ABINet复盘
  2.pipeline修正
  3.DAN论文阅读
疑问：
  1.BERT问答任务中Q&A是同时还是分别通过模型？（同时输入，特殊标记进行分隔）
  2.DAN怎么识别一维和二维？（改变FE和CAM中卷积操作的步长，DAN可以在一维和二维形式切换）
  3.卷积对齐阶段？

DAN模型描述
  输入：表示场景文本图像的二维矩阵
  中间层：视觉特征提取、卷积对齐、文本解码
  输出：多个连续字符组成的序列/字符串/线性表

## 7.26学习记录
BERT使用WordPiece将文本分割成词组列表
对于每个词组，查找它在词汇表中的索引位置
对于每个索引位置，查找对应的词向量
对于不在词汇表中的词组，使用特殊标记（如“UNK”）的词向量代替
使用特殊标记来表示句子的开始（CLS）和分隔（SEP）
 
transformer
编码器输入：文本/字符序列
中间层：词嵌入、自注意力层、前馈神经网络
在每一层中，自注意力层将输入序列中的每个单词或字符与其它单词或字符进行比较，计算单词或字符之间的相关性得分，并加权求和得到每个单词或字符的表示。
解码器输入：目标序列的前一个单词或字符
在每一层对比编码-解码的输出
输出：经过解码器最后一层的线性变换层得到的概率分布，表示预测下一个单词或字符的概率//向量

总输入是单词或字符序列，输出是下一个单词或字符的概率分布或语义向量

对齐   输入的文本图像序列与对应的标签序列进行匹配
将图像中的像素（二维矩阵）与其对应的目标字符（一位数组）进行关联

本周进度：
  1.transformer详细学习（QKV、输入输出）
  2.DAN、BERT复盘
  3.吴恩达深度学习视频学习
疑问：
  1.为什么要除以根号dk？（为了防止出现梯度消失。当dk较大时，QK方差较大，结果差距可能很大）


BERT词级别标记进行词向量转换：
WordPiece将文本分割成词组列表->查找词汇表索引位置->查找对应词向量->添加特殊标记[CLS]和[SEP]
对于不在词汇表中的词组，使用特殊标记的词向量代替

transformer的输入输出：
输入 单词或字符序列
输出 下一个单词或字符的概率分布或语义向量

QKV：
Q(Query)   K(Key)   V(Value)
Q计算与K的相关性/相似度，通过softmax得到一组权重，与对应V乘积求和得到输出
 

DAN对齐：
输入的文本图像序列与对应的标签序列进行匹配
将图像中的像素（二维矩阵）与其对应的目标字符（一位数组）进行关联

一个Q 由上一个标签的结果得到一个Q 与前t-1

## 8.9学习笔记
合成数据集常用于训练，真实集常用于测试

MJSynth (MJ) 合成数据集
SynthText (ST) 2016 CVPR：合成数据集，在80万张图片中人工加入了800万个文本，不是很生硬的叠加，而是作了一些处理，使文字在图片中看起来比较自然
IC03/ IC13/ IC15 2003/2013/2015 ICDAR自然场景图像，英文，标记文本边界框，约1000张
SVT（Street View Text）2012 街景图像，有标注，350张
IIIT 5k words互联网图像，主要包括广告牌，招牌，门牌号，门牌，电影海报等，5000张
CUTE（Curve Text）含弯曲文本的图像，有标注，80张

规则数据集IIIT5K，SVT，IC03和IC13，不规则数据集IC15，SVTP，CT
ICDAR 2017 RCTW 大赛中文识别数据集RCTW（Reading Chinese Text in the Wild），包含一万多张含中文文本的自然场景图片
COCO-Text 2017 ICDAR Robust Reading Challenge实景采集图片，有标注，63686个样本

本周进度：
  1.ABI和DAN实验部分详细阅读
  2.OSTR学习
  3.继续看吴恩达深度学习视频

问题：
  1.消融实验（类似于控制变量法/各模块检验？）
  2.数据集说明-具体实施细节-消融实验及结果-（错误分析/局限性）

数据集	大小（图片数）	来源	文本属性
MJ	/	合成	/
ST	80w	合成	/
IC13	462（229+233）	自然场景	规则
IC15	1500（1000+500）	自然场景	不规则
SVT	350（249+101）	自然场景	规则
SVT-P	639	自然场景	不规则
IIIT5k	5000（2000+3000）	自然场景	规则
CUTE80	80（0+80）	自然场景	不规则

数据集	大小	来源	文本属性
IAM	747+336（文档）	LOB语料库	手写英语
RIMES	1500+100（段落）	信件	手写法语
①	这些数据集用于训练和评估OCR模型，验证使用本文方法将图像转换为文本是否正确
②	复杂多样、规模较大
③	来源真实自然场景，是广泛使用的数据，分为训练集和测试集，每个图像都有对应的标签标注
